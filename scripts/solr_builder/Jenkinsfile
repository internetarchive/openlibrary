pipeline {
  agent {
    dockerfile {
      filename 'Dockerfile'
      dir 'scripts/solr_builder'
      // Needed to make docker-in-docker work
      args '-v /var/run/docker.sock:/var/run/docker.sock'
      args '-v /var/lib/docker/volumes/jenkins-data:/var/lib/docker/volumes/jenkins-data'
      // Where to store dumps
      args '-v /storage:/storage'
    }
  }
  parameters {
    string(name: 'PARTIAL_IMPORT_SIZE', defaultValue: '', description: 'If set to a number, will only import that many records from the dump (useful for testing!)')
    booleanParam(name: 'REUSE_POSTGRES', defaultValue: false, description: 'If true, assumes a functioning postgres is running/correct')
    booleanParam(name: 'INTO_SOLR_FROM_DUMP', defaultValue: true, description: 'If true, reindexes dump into solr')
    booleanParam(name: 'INDEX_WORKS', defaultValue: true, description: 'If true, reindexes works into solr')
    booleanParam(name: 'INDEX_ORPHANS', defaultValue: true, description: 'If true, reindexes orphans into solr')
    booleanParam(name: 'INDEX_SUBJECTS', defaultValue: true, description: 'If true, reindexes subjects into solr')
    booleanParam(name: 'INDEX_AUTHORS', defaultValue: true, description: 'If true, reindexes authors into solr')
    string(name: 'MAX_CORES', defaultValue: '12', description: 'Max number of simultaneous cores')
  }
  environment {
    // The *host* location of Jenkins directory
    JENKINS_HOME_DIR = sh(script: "docker volume inspect --format='{{.Mountpoint}}' jenkins-data", returnStdout: true).trim()
    // Where to save dumps
    DUMP_DIR = '/storage/openlibrary'
    // The location of the repo on the host machine; needed for docker-in-docker images
    HOST_SOLR_BUILDER_DIR = "${env.WORKSPACE}/scripts/solr_builder".replaceFirst('/var/jenkins_home', env.JENKINS_HOME_DIR)
  }
  stages {
    stage('1: Create a local postgres copy of the database') {
      when { expression { return !params.REUSE_POSTGRES } }
      environment {
        // Where to download the ol full dump from
        OL_DUMP_LINK = 'https://openlibrary.org/data/ol_dump_latest.txt.gz'
        // Get the date-suffixed name of the latest dump
        FULL_OL_DUMP_FILE = sh(script: "curl '${env.OL_DUMP_LINK}' -s -L -I -o /dev/null -w '%{url_effective}'", returnStdout: true).trim().split('/').last()
        // Nmae of partial dump file
        PARTIAL_OL_DUMP_FILE = "partial_${env.FULL_OL_DUMP_FILE}"
        // Use partial dump file, if specified so in the parameters
        OL_DUMP_FILE = "${params.PARTIAL_IMPORT_SIZE ? env.PARTIAL_OL_DUMP_FILE : env.FULL_OL_DUMP_FILE}"
      }
      stages {
        stage('Setup') {
          parallel {
            stage('Launch postgres containers') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh(label: 'Launch the database container',
                    script: 'docker-compose up -d --no-deps db')

                  sh(label: 'optional; GUI for the database at port 8087',
                    script: 'docker-compose up -d --no-deps adminer')

                  // Wait for postgres
                  sleep 10

                  sh(label: 'Create the table to store the dump',
                    script: 'docker-compose exec -T -u postgres db psql -d postgres -f sql/create-dump-table.sql')
                }
              }
            }
            stage('Setup dump file') {
              stages {
                stage('Download dump file') {
                  steps {
                    dir(env.DUMP_DIR) {
                      sh "wget --progress=dot:giga --trust-server-names --no-clobber ${env.OL_DUMP_LINK}"
                    }
                  }
                }
                stage('Create partial dump file') {
                  when { expression { return params.PARTIAL_IMPORT_SIZE } }
                  steps {
                    dir(env.DUMP_DIR) {
                      sh "zcat ${env.FULL_OL_DUMP_FILE} | head -n ${params.PARTIAL_IMPORT_SIZE} | gzip > ${env.PARTIAL_OL_DUMP_FILE}"
                    }
                  }
                }
              }
            }
          }
        }
        stage('Populate postgres') {
          stages {
            stage('Import dump into postgres') {
              environment {
                PARALLEL_PROCESSES = '6'
              }
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh(label: 'Import dump into postgres',
                    script: "./psql-import-in-chunks.sh ${env.DUMP_DIR}/${env.OL_DUMP_FILE} ${env.PARALLEL_PROCESSES}")
                  
                  waitUntil {
                    script {
                      // Once completed, the log file says "COPY 8828224" (or however many records)
                      // error if we have anything else
                      // (cat at the end to avoid grep exiting when no match)
                      def error_output = sh(script: 'cat logs/psql-chunk-* | grep -v COPY | cat', returnStdout: true)
                      echo "Assert no psql error"
                      assert !error_output
                      
                      def cores_completed = sh(script: 'cat logs/psql-chunk-* | grep COPY | wc -l', returnStdout: true).trim()
                      echo "Completed ${cores_completed} core(s) of ${env.PARALLEL_PROCESSES}"
                      sleep 15*60 // By default this checks every 15s; that's unnecessary for a ~3.5 hour process
                      return cores_completed == env.PARALLEL_PROCESSES
                    }
                  }
                }
              }
              post {
                always {
                  dir(env.HOST_SOLR_BUILDER_DIR) {
                    sh 'for f in logs/psql-chunk-*.txt; do echo "$f: "; cat "$f"; done > logs/psql-import.log'
                    archiveArtifacts artifacts: 'logs/psql-import.log'
                  }
                }
              }
            }
            stage('Test import successful') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  script {
                    def lines_in_dump = sh(script: "cd ${env.DUMP_DIR}; zcat ${env.OL_DUMP_FILE} | wc -l", returnStdout: true).trim()
                    echo lines_in_dump
                    def rows_in_db = sh(script: 'source ./aliases.sh; psql -q -f sql/count-all-rows.sql', returnStdout: true).trim()
                    echo rows_in_db
                    echo "Assert db has same number of rows as ol dump file"
                    assert lines_in_dump == rows_in_db
                  }
                }
              }
            }
            stage('Create postgres indices') {
              steps {
                dir(env.HOST_SOLR_BUILDER_DIR) {
                  sh "docker-compose exec -T -u postgres db psql postgres -f sql/create-indices.sql | ts '[%Y-%m-%d %H:%M:%S]'"
                }
              }
            }
          }
        }
      }
      post {
        always {
          dir(env.DUMP_DIR) {
            sh(label: 'Remove partial dump file (if we had one)',
              script: "rm -f ${env.PARTIAL_OL_DUMP_FILE}")
          }
        }

        unsuccessful {
          dir(env.HOST_SOLR_BUILDER_DIR) {
            sh (label: 'Cleanup postgres',
              script:'docker-compose stop db adminer; docker-compose rm -f db adminer; docker volume rm solr_builder_postgres-data')
          }
        }
      }
    }
    stage('2: Populate solr') {
      stages {
        stage('Restart Postgres') {
          // Have to restart to because the previous workspace was destroyed on cleanup
          when { expression { return params.REUSE_POSTGRES } }
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh(label: 'Stop the db containers',
                script: 'docker-compose stop db adminer')

              sh(label: 'Start the db containers',
                script: 'docker-compose up -d --no-deps db adminer')
            }
          }
        }
        stage('Setup') {
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh(label: 'Launch solr',
                script: 'docker-compose up --no-deps -d solr')

              sh(label: 'Build "lite" ol environment',
                script: 'docker-compose build --pull ol')
              
              sh(label: 'Build the cython files',
                script: 'docker-compose run --rm ol ./build-cython.sh')
            }
          }
        }
        stage('Reindexing') {
          parallel {
            stage('Reindex from ol dump') {
              when { expression { return params.INTO_SOLR_FROM_DUMP } }
              stages {
                stage('Insert works') {
                  when { expression { return params.INDEX_WORKS } }
                  steps {
                    dir(env.HOST_SOLR_BUILDER_DIR) {
                      sh "CHUNK_ETA=70 docker-compose run --name ol_run_works --rm -T ol ./index-type.sh work ${params.MAX_CORES} works"
                      solr_commit('solr')
                    }
                  }
                  post {
                    always {
                      dir(env.HOST_SOLR_BUILDER_DIR) {
                        // Save Progress Files
                        sh '''tail --quiet -n1 $(ls -tr progress/works/*) | gzip > progress/works-progress.txt.gz'''
                        archiveArtifacts artifacts: "progress/works-progress.txt.gz"
                        // Save Log Files
                        sh '''cat $(ls -tr logs/works/*) | gzip > logs/works-indexing.log.gz'''
                        archiveArtifacts artifacts: "logs/works-indexing.log.gz"
                      }
                    }
                  }
                }
                stage('Insert orphans') {
                  when { expression { return params.INDEX_ORPHANS } }
                  steps {
                    dir(env.HOST_SOLR_BUILDER_DIR) {
                      sh "CHUNK_ETA=35 docker-compose run --name ol_run_orphans --rm -T ol ./index-type.sh orphan ${params.MAX_CORES} orphans"
                      solr_commit('solr')
                    }
                  }
                  post {
                    always {
                      dir(env.HOST_SOLR_BUILDER_DIR) {
                        // Save Progress Files
                        sh '''tail --quiet -n1 $(ls -tr progress/orphans/*) | gzip > progress/orphans-progress.txt.gz'''
                        archiveArtifacts artifacts: "progress/orphans-progress.txt.gz"
                        // Save Log Files
                        sh '''cat $(ls -tr logs/orphans/*) | gzip > logs/orphans-indexing.log.gz'''
                        archiveArtifacts artifacts: "logs/orphans-indexing.log.gz"
                      }
                    }
                  }
                }
                stage('Insert subjects') {
                  when { expression { return params.INDEX_SUBJECTS } }
                  steps {
                    dir(env.HOST_SOLR_BUILDER_DIR) {
                      sh """
                      for subject_type in subject person place time; do
                        docker-compose run --name ol_run_subjects_\$subject_type --rm -T ol \
                          python ./solr_builder/index_subjects.py \
                            \$subject_type \
                            --chunk-size 10000 \
                            --instances ${params.MAX_CORES} \
                            --solr-base-url 'http://solr:8983/solr/openlibrary' \
                            --skip-id-check
                      done
                      """
                      solr_commit('solr')
                    }
                  }
                }
                stage('Insert authors') {
                  when { expression { return params.INDEX_AUTHORS } }
                  steps {
                    dir(env.HOST_SOLR_BUILDER_DIR) {
                      sh "docker-compose run --name ol_run_authors --rm -T ol ./index-type.sh author ${params.MAX_CORES} authors"
                      solr_commit('solr')
                    }
                  }
                  post {
                    always {
                      dir(env.HOST_SOLR_BUILDER_DIR) {
                        // Save Progress Files
                        sh '''tail --quiet -n1 $(ls -tr progress/authors/*) | gzip > progress/authors-progress.txt.gz'''
                        archiveArtifacts artifacts: "progress/authors-progress.txt.gz"
                        // Save Log Files
                        sh '''cat $(ls -tr logs/authors/*) | gzip > logs/authors-indexing.log.gz'''
                        archiveArtifacts artifacts: "logs/authors-indexing.log.gz"
                      }
                    }
                  }
                }
              }
              post {
                always {
                  dir(env.HOST_SOLR_BUILDER_DIR) {
                    sh(label: 'Stop/rm running indexers',
                      script: '''
                      docker stop $(docker container ls -f "name=ol_run" -q) || true
                      docker rm $(docker container ls -f "name=ol_run" -aq) || true
                      ''')
                  }
                }
//                 unsuccessful {
//                   dir(env.HOST_SOLR_BUILDER_DIR) {
//                     sh(label: 'Remove solr service',
//                       script:'docker-compose stop solr; docker-compose rm -f solr; docker volume rm solr_builder_solr-data')
//                   }
//                 }
              }
            }
          }
        }
        stage('Optimize indices') {
          // This is kind of like a disk defrag, but it's ESSENTIAL after a big import. Shouldn't
          // really be needed after that.
          steps {
            dir(env.HOST_SOLR_BUILDER_DIR) {
              sh "docker-compose exec -T solr curl -s http://localhost:8983/solr/openlibrary/update?optimize=true&maxSegments=1"
            }
          }
        }
      }
    }
  }
  post {
    always {
      deleteDir() // Delete the workspace
    }
  }
}

String solr_query_count(String solr_name, String type_to_count) {
  def solr_url = "http://localhost:8983/solr/select/?q=type%3A${type_to_count}&rows=0"
  def count_cmd = "docker-compose exec -T ${solr_name} curl -s '${solr_url}' | xmlstarlet sel -t -v '//result/@numFound'"
  def count = sh(script: count_cmd, returnStdout: true).trim()
  return count
}

void solr_commit(String solr_name) {
  sh(label: 'Solr commit the changes',
    script: "docker-compose exec -T ${solr_name} curl http://localhost:8983/solr/openlibrary/update?commit=true")
}
